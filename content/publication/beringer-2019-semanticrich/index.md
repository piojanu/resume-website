---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Towards Semantic-Rich Word Embeddings
subtitle: ''
summary: ''
authors:
- Grzegorz Beringer
- Mateusz Jablonski
- Piotr Januszewski
- Andrzej Sobecki
- Julian Szymanski
tags: []
categories: []
date: '2019-01-01'
lastmod: 2021-09-16T17:17:19+02:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-09-16T15:17:19.112694Z'
publication_types:
- '1'
abstract: 'In recent years, word embeddings have been shown to improve the performance in NLP tasks such as syntactic parsing or sentiment analysis. While useful, they are problematic in representing ambiguous words with multiple meanings, since they keep a single representation for each word in the vocabulary. Constructing separate embeddings for meanings of ambiguous words could be useful for solving the Word Sense Disambiguation (WSD) task. In this work, we present how a word embeddings average- based method can be used to produce semantic-rich meaning embeddings, and how they can be improved with distance optimization techniques. We also open-source a WSD dataset that was created for the purpose of evaluating methods presented in this research.'
publication: '*Proceedings of the 2019 Federated Conference on Computer Science and
  Information Systems, FedCSIS 2019, Leipzig, Germany, September 1-4, 2019*'
doi: 10.15439/2019F120
---
